\chapter{Automatic error control during forward flux sampling of rare events in master equation models}

\section{Introduction}

%Advances in single-cell and single-molecule techniques continue to produce new details about the individual biomolecules \supercite{Liu:2015jh,Vera:2016gm,Locke:2009gx,Muraro:2016hs,Li:2011kz}. Likewise, modeling efforts have progressed toward the goal of reconstructing complete cellular networks\supercite{Macklin:2014et,Karr:2015ej}. However, work is still needed to understand the dynamics of cells

Understanding how inanimate biochemical molecules come together and interact to form a living cell is one of the fundamental goals of biology\supercite{Harold:2003uu}. The cell's state can be described as a point in a high dimensional space, where each dimension corresponds to the concentration of a different molecule. Complex, nonequilibrium regulatory and signaling networks connect these molecules through positive and negative interactions, naturally resulting in a large number of metastable states within the space\supercite{Zhou:2012fc},  representing different cellular phenotypes. Projected into two dimensions, the phase space appears as a rough landscape, sometimes referred to as a quasipotential\supercite{Zhou:2012fc}, epigenetic\supercite{Waddington:1957ub}, or phenotypic\supercite{Nichols:2011cb} landscape. Stochastic fluctuations cause the cell's state to move along the landscape and occasionally jump between metastable states.

%It is now possible to describe the entire biochemical network (\ie the complete set of interacting pathways) underlying many different cellular activities.

%In particular, stochastic modeling is able to accurately capture the effects of fluctuation and variation (\ie intrinsic and extrinsic noise) on network behavior\supercite{Roberts:2011cs}. This makes stochastic modeling well suited to the simulation of switchable systems such as gene regulatory networks (GRN)\supercite{Hinman:2014bo,Borotkanics:2014hw,Barnes:2016gm,Gardner:2000bm}.

%As a tradeoff for its high accuracy and single molecule resolution, stochastic modeling is computationally expensive\supercite{Gillespie:2007bx}. Rare events (events for which the probability of occurrence is much lower than the most likely events, typically by several orders of magnitude) in particular pose a computational challenge for stochastic simulations\supercite{Becker:2012ej}. In the context of biochemical networks, rare events typically arise from the metastable dynamics of the collective action of many elementary reactions\supercite{Baron:2017tf}. For example, one common rare event is the switch from a state of low expression protein expression to a state of high protein expression\supercite{Choi:2010ge}. This kind of switching event can be found in many different genetic regulatory networks (GRN), such as those underlying the lysis/lysogeny decision in phage $\lambda$\supercite{Cao:2010dp} and the \it{lac} operon in \it{E. coli}\supercite{Roberts:2011cs}. A great number of elementary reactions, such as binding/unbinding of various species, are typically required before enough transcription factor is activated to shift the GRN towards the high expression state\supercite{Becskei:2005cs}. Each computational step of a stochastic simulation corresponds to a single elementary reaction\supercite{Gillespie:1976bj}. Thus, on average a great many computational steps are needed in order to observe a single rare event in a simulation trajectory.

Metastable systems, because they depend upon random fluctuations, are typically modeled using a formulation of the chemical master equation\supercite{Qian:2010if,Roberts:2015iu} or using stochastic differential equations\supercite{Wu:2013dx}. In the former case, the models are often numerically studied using the stochastic simulation algorithm\supercite{Gillespie:1976bj,Gillespie:2007bx} (SSA) or one of its many varieties\supercite{Bratsun:2005cs,Cao:2004cu,Gibson:2000jq}. Such simulations have provided insight into diverse biological processes, including: the lysis/lysogeny decision in bacteriophage $\lambda$\supercite{Cao:2010dp}, the \textit{lac} operon in \textit{Escherichia coli}\supercite{Roberts:2011cs},  check-pointing during the cell cycle\supercite{Li:2014iw}, differentiation of stem cells\supercite{Zhang:2014bu}, the binding of an intrinsically disordered peptide to a protein\supercite{Zwier:2016fx}, macrophage regulation\supercite{Smith:2016gj}, and gradient detection during yeast mating\supercite{Sharma:2016gw}.

%Although a number of approximation methods have been developed for skipping over the work of simulating the interstitials, for a long time it was thought that the accurate stochastic simulation of rare events required the simulation of every common event in between. Recently a class of advanced statistical techniques have emerged that promise to speed up the simulation of biochemical networks that include rare events. Collectively these techniques are known as \abr{ES}.  \abr{ES} can be thought of broadly as an extension of rare event sampling techniques, such as umbrella sampling, into the nonequilibrium domain (to which most biochemical networks belong). All \abr{ES} methods proceed along the same basic lines. First, the state space of the system (\eg the possible copy numbers of the biochemical species) under study is subdivided into a set of bins. Many separate simulations are then executed with starting and stopping conditions based on the boundaries of those bins, with an eye towards biasing the overall occupancy of said simulations towards regions of interest in the state space. Advanced statistical techniques are then used to unweight the collective simulation outcomes so as to calculate results equivalent to those obtained via unbiased simulation.

Transitions between metastable states in stochastic biochemical systems are infrequent in that one must wait a long time to observe a large fluctuation that causes the system to switch states\supercite{Baron:2017tf}. The time spent in the transition region is also very short relative to the waiting time\supercite{Becker:2012ej}. Such dynamics are known as rare events, and are expensive to simulate as most of the computational effort is spent simply simulating the waiting state. Numerical methods for improving the efficiency of simulating rare events, generally known as \abr{ES} techniques, have a long history. The earliest work in the field is typically credited to Kahn \etal{}\supercite{Kahn:1951es}, but has since been applied to the study of many systems in molecular mechanics. The key assumption is that metastable systems exhibit a large barrier separating the states on some free energy landscape. By biasing the simulation toward the transition path between the states one can more efficiently recover its free energy profile and, thus, the transition rates. For example: umbrella sampling\supercite{Torrie:1977hs,Souaille:2001gm} uses overlapping biasing potentials to confine multiple simulations to narrow windows along the path; metadynamics\supercite{Laio:2002ft,Dama:2014ef} gradually adds a repulsive potential to low points on the free energy surface causing the system to explore low probability regions of phase space and to eventually cross the barrier; transition path sampling\supercite{Dellago:1998kw,Crooks:2001cf,Jung:2017jj} generates a statistically correct set of transition paths starting from an initial trial path using acceptance and rejection criteria; weighted ensemble\supercite{Huber:1996dna,Zuckerman:2017eq} runs multiple independent trajectories while dynamically splitting and merging them, with careful accounting of the trajectory weights, to balance simulations along the transition path. In general, a final unbiasing and/or recombination step is always needed to calculate unbiased statistics.

Although stochastic biochemical systems are described by quasipotential rather than free energy landscapes, the underlying physics is compatible with \abr{ES}. Consequently, many varieties of \abr{ES} can be applied to stochastic biochemical systems with rare event dynamics. Three of the most well known candidates are \abr{FFS} \supercite{Allen:2005wy,Allen:2006cp,Valeriani:2007hv,Allen:2009kb}, nonequilibrium umbrella sampling\supercite{Warmflash:2007dz,Dickson:2009gt,Dickson:2009fua}, and weighted ensemble\supercite{Zhang:2010kfa,Bhatt:2010df,Adelman:2013ii,Donovan:2013gz,Donovan:2016bi}. Although much discussion has taken place regarding the strengths and weaknesses of each of these methods\supercite{Dickson:2010gf,Escobedo:2009dya} there is no consensus as to an optimal approach. In the case of a transition between only two metastable states along a single order parameter, \abr{FFS} appears to be a reasonable choice and is the focus of this work.

The \abr{FFS} method can be used to calculate both the transition rate, \ie, the inverse of the \abr{MFPT}, between two metastable states and the probability distribution along the transition path\supercite{Valeriani:2007hv}. Although it can be applied to non-stationary processes\supercite{Becker:2012fl}, here we investigate only stationary processes. The fundamental operation of \abr{FFS} is to partition the phase space along an order parameter using a series of non-intersecting interfaces that track progress over the transition barrier. Many trial trajectories are started successively from each interface in order to compute the probability of advancing to the next interface versus returning to the initial state. The product of the advancement probabilities and the probability flux out of the initial state gives the mean transition rate.

%The current literature leaves open many questions about the formal mathematics and error statistics of \abr{ES}. Of these open questions, the most essential is this: when accuracy is controlled for, is \abr{ES} truly faster than \abr{DS} (\ie the standard stochastic simulation method)? If so, for what models, and under what conditions? We sought to develop answers for these questions in the form of mathematical proofs, focusing our efforts on the \abr{FFS} method. Our strategy was to find formal expressions for the computational effort required to produce a simulation of a given accuracy for both the \abr{DS} and \abr{FFS} methods, and then draw conclusions from a comparison of the two.

%All existing studies looking at the relative speedup of \abr{ES} simulation over \abr{DS} simulation either do not control for accuracy\supercite{Allen:2005wy}, or are based on empirical results from simulations of one or two model systems that can only be applied narrowly to those specific systems\supercite{Donovan:2013gz}, or both. Previous work done by Allen and coworkers is suggestive of a possible path towards a more general statement about speedups. They found an analytical expression that can be used to predict the variance of the result (a proxy for simulation error) of an \abr{FFS} simulation given the computational effort expended in each of an \abr{FFS} simulation’s separate phases. In order to answer our question of interest about the speed of of \abr{FFS} simulation, the next step is to find a way to predict the computational effort required to produce a simulation of a given accuracy, or in other words the inverse of Allen’s expression. However, there are many possible equations for the inverse, as there are many ways to distribute computational effort amongst the various phases of an \abr{FFS} simulation such that the same simulation error is achieved. This is an example of the classical inverse problem that occurs with overdetermined equations in many fields of physics.

The generation of many trial trajectories at each of the interfaces is a large part of the computational work in \abr{FFS}. It is not surprising, then, that a number of authors have tried to optimize performance of \abr{FFS} by studying the interplay between the number of trajectories sampled and the statistical error. Allen {\etal}\supercite{Allen:2006ch} first introduced a framework for studying the relationship between error in the estimated transition rate constant and error in the estimated interface trial probabilities. They also studied the computational efficiency of \abr{FFS}, taken to be the inverse of cost times error, and showed that it was relatively insensitive to the choice of interface parameters. Borrero and Escobedo\supercite{Borrero:2008il} presented two techniques to minimize \abr{FFS} error for a fixed cost either by optimizing the number of trial runs at each interface or by iteratively refining the placement of the interfaces. Kratzer {\etal}\supercite{Kratzer:2013fs} extended this idea with an algorithm to automatically define interface number and position on-the-fly by constraining the number of successful trials to be the same for each interface. However, none of these previous works included a systematic treatment of the error arising from the estimate of the flux out of the initial state and they all assume that computational cost per unit distance along the order parameter is fixed. Jian {\etal}\supercite{Jiang:2013jc} later showed that interface placement along a complex transition landscape, {\eg} one with a metastable intermediate state, cannot be correctly optimized by methods that assume cost is proportional to interface distance.

%Using a novel derivation, we found a generalization of Allen's $\left( \mathsf{effort} \rightarrow \mathsf{error} \right)$ relation that predicts the complete distribution of the simulation result, not just the variance\footnote{Further, our version of the expression is more complete, as it includes the contributions of an important source of error (sampling error in the initial \abr{FFS} phase) that Allen did not consider.}. We worked around the inverse problem by using variational calculus to find the unique inverse of our expression that gives the minimum computational effort required to achieve a given level of error in a simulation. The ultimate product of this mathematical work is a set of simple, closed-form equations that we collectively call the \abr{FFS} Optimizing Equation (FFSOE). The FFSOE can be used to determine the phase-by-phase simulation plan that will achieve a given error goal (in terms of a desired upper bound on the percent error of the simulation result) using the least possible computational effort. We derived a number of subsequent results from the FFSOE. Most importantly, we define an exact mathematical criterion under which it is possible for \abr{FFS} to be faster than \abr{DS}, and show how to calculate the actual speedup factor.

%We also introduce \abr{FFPilot}, a novel \abr{ES} method based around the FFSOE. Compared to standard \abr{FFS} simulations, \abr{FFPilot} simulations run faster and are easier to set up. \abr{FFS} requires a user to specify a large set of unconstrained parameters (the number of trajectories to run in each separate phase) with no particular guidance. In place of these many parameters, \abr{FFPilot} requires only a single error goal, from which the runs per phase are determined in an automated and optimized way. We present the results of extensive testing of our parallelized implementation of \abr{FFPilot} (written in C++ using the MPI parallelization framework) on several different model systems of physical and biological processes. For a given level of simulation accuracy, we show that the computational effort that the FFSOE specifies either exactly matches the effort that is actually required, or is a monotonic bound on the effort, depending on the model being simulated. We show that a form of landscape correlation error is sufficient to explain the anomalous extra error that is not predicted by the FSSOE.

Here we present a method to optimally perform an \abr{FFS} simulation of a nonequilibrium stationary process at a given margin of error and confidence interval. The key advance of our method is to estimate all of the interface probabilities and costs from a short pilot simulation and then use those parameters to optimize the configuration of a full \abr{FFS} simulation. We minimize the total computational cost to achieve a user specified statistical error, which greatly reduces the number of choices that need to be made by the user. Our method includes a new formal treatment of the sampling error arising in the calculation of the flux out of the initial state, which is shown to have a significant influence on total cost. Unlike previous efforts, the cost in our method varies by interface, which enables it to account for changes in computational efficiency along the order parameter. We evaluate the capability of our method to control sampling error on three different models exhibiting rare event dynamics. In two one-dimensional models, sampling error dominates and is controlled precisely while in a multidimensional model landscape error becomes significant and requires oversampling. Finally, we derive an expression for the speedup of \abr{FFS} relative to direct simulation for the same level of error and show that the advantage of \abr{FFS} is substantial and increases with the rarity of the event.

The remainder of this work is organized as follows: In \secref{sec:methods} we introduce our theoretical framework for estimating the sampling error in a \abr{FFS} simulation. In \secrefTwo{sec:opteq_derivation}{sec:ffpilot_definition} we derive the optimizing equation used by our method to control sampling error and then describe the \abr{FFPilot} algorithm. In \secrefRange{sec:rem}{sec:gts} we evaluate the accuracy and performance of our method using three increasingly complex models. In \secrefTwo{sec:landscape_error}{sec:oversampling} we analyze sources of error outside of sampling error. Finally, in \secrefTwo{sec:efficiency}{sec:speedup} we evaluate the theoretical efficiency and measure the performance of \abr{FFPilot}.


%As the experimental picture of the cell grows to accommodate data from more and more different kinds of single cell and single molecule experiments, it becomes more and more important that theoretical models achieve a commensurate level of accuracy\supercite{Macklin:2014et,Karr:2015ej}.
%
%The field of systems biology offers many choices for such a theorertical basis, each with their own strengths and weaknesses. Broadly speaking, there exists a spectrum of cell modeling techniques that run 
%
%In particular, stochastic modeling is able to simulate transitions that are dominated by the dynamics of low copy number species (in the case of GRNs, the transcription factors) in a theoretically rigorous fashion\supercite{Cao:2010dp,Golding:2015ft,Robb:2014io,Arkin:1998vy}. This is in contrast to most other available modeling methods (such as those based on deterministic differential equations).
%
%As theoreticians, we would ideally like to be able to build stochastic models of very large biochemical networks involving thousands of distinct chemical species and reactions. Unfortunately, stochastic simulation is very slow \todo{cite}, often to the point of impracticality. The standard stochastic simulation protocol is known as the Gillespie Algorithm \todo{}, or more eponymously as the Stochastic Simulation Algorithm (SSA). SSA is straightforward to use and is extremely accurate. However, even simulations of simple 2-state GRNs with fewer than 10 distinct biochemical species may require the equivalent of CPU years worth of computational effort. This is due to the large separation of the time scales between individual molecular events and the switching rate of the overall system (see Fig \todo{figure}). Essentially, in order to simulate what may be the relatively slow dynamics of a system as a whole, SSA must simulate every single molecular event, no matter how many. Thus, standard SSA is too slow for the study of many complex gene regulatory networks of interest. 
%
%One of the greatest strengths of the \abr{ES} approach is that it requires no new approximationsUnlike methods such as Tau Leaping that
%
%We began this study with the goal in mind of implementing a parallelized \abr{ES} framework within Lattice Microbes (LM). LM is an established stochastic simulation software package maintained by the Roberts Lab, and has been extensively optimized for running on
%both single machines and HHPC cluster environments. We began with FFlux, as this method seemed the most compatible with our existing framework. However, as we designed and tested several prototype implementations of FFlux in LM, we noticed a recurrent problem. When using simulations to estimate values such as a \abr{MFPT},  first  Whereas the error statistics of the traditional stochastic simulation methods
%
%In general, when running simulations using the traditional stochastic modeling methods, the statistics of the sampling error closely follow those of the equivalent in vivo experiment. For example, when determining the \abr{MFPT} between two states of a system, if the researcher decides that they want an answer accurate to within $\pm 1\%$ the number of transition event observations required can be estimated using a straightforward interval based approach.
%Building on previous work\supercite{Allen:2006ch,Borrero:2008il,Borrero:2009wj,Kratzer:2013fs}, Further, we use the FFlux optimization equation to prove that FFlux is
%
%To our knowledge, 
%
%We have developed a novel \abr{ES} method that we call \abr{FFPilot}. \abr{FFPilot} automatically formulates a simulation plan that minimizes computational effort while achieving a user-defined error goal.
%
%Using a novel derivation, we found a generalization of Allen's effort$\rightarrow$error relation that predicts the margin of error (\ie the confidence interval of the percent error 
%
%a version of our effort$\rightarrow$error relation that constrains the free parameters to the unique global minimum with respect to 
%
%From this starting point used techniques from variational calculus to find an expression for the minimum computational effort required to perform a simulation of a specified accuracy.  we determined an exact set of mathematical conditions under which \abr{FFS} simulation is faster than \abr{DS} simulation. As well,
%
%The current work focuses on the relationship between computational effort and simulation accuracy for both \abr{DS} (the standard stochastic simulation method) and \abr{FFS} (FFlux). The papers describing the various \abr{ES} methods all make claims that \abr{ES} can speed up simulations by at least several orders of magnitude (one Weighted Ensemble paper claims speedups as large as ${10}^{15}$ fold). However, Once error is controlled for, the question remains open: is \abr{ES} faster than \abr{DS} for all model systems under all simulation conditions? If not for all, then when. There is no mathematical proof in the extant literature that  simulation accuracy is proportional to computational effort for all types of stochastic simulation,  typically by a factor of several orders of magnitude (one paper claims speed ups ) In standard FFlux simulation, the count of independent
%
%Earlier work on FFlux error was done by Allen and coworkers\supercite{Allen:2006ch}. They showed how the computational effort expended in each phase of an FFlux simulation relates to the error of the overall simulation result, although their treatment only covers a subset of the phases of an FFlux simulation. Building on Allen's results, Borrero and coworkers derived a recursive-form expression that can determine a simulation plan that minimizes the error in the final result given a fix quantity of computational effort available to "spend" on the simulation.
%
%We have derived expressions relating error and computational effort in FFlux simulations, and have developed a new \abr{ES} variant that incorporates these expressions. We rederived Allen's result from first statistical principles, and in so doing found a generalization of the computational effort/error relation that covers all of the simulation phases.

